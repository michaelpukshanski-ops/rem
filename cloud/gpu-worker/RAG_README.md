# ğŸ§  REM Local RAG System

**Query your voice transcriptions using natural language - 100% local, 100% free!**

Ask questions like:
- "What did I say about AWS deployment?"
- "When did I mention the project deadline?"
- "Summarize all my thoughts on the new feature"

Get AI-powered answers with source citations, all running on your Mac Mini M4.

## âœ¨ Features

- ğŸ” **Semantic Search** - Find by meaning, not just keywords
- ğŸ’¬ **Natural Language** - Ask questions in plain English
- ğŸ¤– **Local LLM** - Llama 3.2 runs on your Mac (no cloud!)
- ğŸ“š **Source Citations** - See which recordings answers came from
- âš¡ **Fast** - Optimized for Apple Silicon M4
- ğŸ’° **Free** - Zero API costs, zero cloud fees
- ğŸ” **Private** - Your data never leaves your machine

## ğŸš€ Quick Start

```bash
# 1. One-time setup (~10 minutes)
./scripts/setup-rag-system.sh

# 2. Sync your transcripts from S3
cd cloud/gpu-worker && source venv/bin/activate
python3 scripts/sync-transcripts.py

# 3. Index them for search
python3 src/indexer.py

# 4. Start asking questions!
python3 src/query_memory.py "What did I say about AWS?"

# Or use interactive mode
python3 src/query_memory.py --interactive
```

## ğŸ“‹ What You Need

- Mac Mini M4 (or any Apple Silicon Mac)
- macOS 12.0+
- 10GB free disk space
- Homebrew installed

## ğŸ¯ How It Works

```
Your Voice â†’ Transcription â†’ S3 Storage
                                â†“
                         Local Sync
                                â†“
                    Embedding Generation
                                â†“
                    ChromaDB Vector Store
                                â†“
            Natural Language Query â†’ Semantic Search
                                â†“
                    Retrieved Context â†’ Local LLM
                                â†“
                    Answer with Sources!
```

## ğŸ’¡ Example Queries

```bash
# Find specific information
python3 src/query_memory.py "What did I decide about the database?"

# Time-based queries
python3 src/query_memory.py "What did I discuss last week?"

# Summarization
python3 src/query_memory.py "Summarize my thoughts on the project"

# Action items
python3 src/query_memory.py "What action items did I mention?"

# Interactive mode for multiple questions
python3 src/query_memory.py --interactive
```

## ğŸ“š Documentation

- **[Setup Guide](../../docs/RAG_SETUP_GUIDE.md)** - Detailed installation and usage
- **[Quick Reference](../../docs/RAG_QUICK_REFERENCE.md)** - Common commands and tips
- **[System Design](../../docs/RAG_SYSTEM_DESIGN.md)** - Technical architecture

## ğŸ”§ Components

### Scripts
- `scripts/sync-transcripts.py` - Download transcripts from S3
- `src/indexer.py` - Create embeddings and index
- `src/query_memory.py` - Query interface

### Configuration
- `src/rag_config.py` - System configuration
- `requirements-rag.txt` - Python dependencies

### Storage
- `~/.rem/transcripts/` - Local transcript cache
- `~/.rem/chroma/` - Vector database
- `~/.rem/sync_metadata.json` - Sync state

## ğŸ”„ Keeping Updated

### Manual Sync
```bash
# Run periodically to get new transcripts
python3 scripts/sync-transcripts.py
python3 src/indexer.py
```

### Automatic Sync
The GPU worker auto-indexes new transcripts! Just keep it running:
```bash
python3 src/worker.py
```

## ğŸ’° Cost Breakdown

| Component | Cost |
|-----------|------|
| Ollama (LLM) | **$0** - Free & open source |
| ChromaDB | **$0** - Free & open source |
| Embeddings | **$0** - Free & open source |
| S3 Download | **~$0.001** - One-time, pennies |
| **Total** | **$0/month** âœ… |

Compare to:
- OpenAI GPT-4: ~$0.03 per 1K tokens
- Pinecone Vector DB: $70/month minimum

## ğŸ“Š Performance

On Mac Mini M4:
- **Sync**: ~100 transcripts/minute
- **Indexing**: ~50 transcripts/minute
- **Query**: ~1-2 seconds
- **Memory**: ~1-2GB RAM

## ğŸ” Privacy

- âœ… Everything runs locally
- âœ… No data sent to cloud AI services
- âœ… Transcripts stay on your machine
- âœ… Open source components

## ğŸ†˜ Troubleshooting

```bash
# Ollama not running?
brew services start ollama

# Collection not found?
python3 src/indexer.py

# No transcripts?
python3 scripts/sync-transcripts.py

# Need help?
python3 src/query_memory.py --help
```

## ğŸ“ Tips

1. Use **interactive mode** for multiple queries
2. Keep **worker running** for auto-indexing
3. **Sync regularly** to get new transcripts
4. Use **specific questions** for better results
5. **Check sources** to verify information

## ğŸš€ Advanced Usage

```bash
# Re-index everything
python3 src/indexer.py --reindex

# Get more results
python3 src/query_memory.py "question" --top-k 10

# Search only (no LLM)
python3 src/query_memory.py "search term" --search-only

# Verbose logging
python3 src/query_memory.py "question" --verbose
```

## ğŸ¤ Contributing

This is part of the REM (Recording & Memory) system. See main README for more info.

## ğŸ“„ License

Same as REM project.

---

**Built with:**
- [Ollama](https://ollama.ai/) - Local LLM runtime
- [ChromaDB](https://www.trychroma.com/) - Vector database
- [Sentence Transformers](https://www.sbert.net/) - Embeddings
- [Llama 3.2](https://ai.meta.com/llama/) - Language model

**Enjoy your local AI memory system! ğŸ§ **

